{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Afra10/Classification-and-Captioning-Aircraft-Damage-Using-Pretrained-Models/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISEX6boG_L0E"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Required Libraries"
      ],
      "metadata": {
        "id": "oA6at2O8AKYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.applications import VGG16\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "import random\n"
      ],
      "metadata": {
        "id": "k1bVD9BSAHi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for reproducibility\n",
        "seed_value = 42\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "tf.random.set_seed(seed_value)"
      ],
      "metadata": {
        "id": "Ip4GZSWkAnmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Preparation"
      ],
      "metadata": {
        "id": "s290RC3VAu8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the batch size,epochs\n",
        "batch_size = 32    # CHANGED: Smaller batch size\n",
        "n_epochs =5     # CHANGED: More epochs (but with early stopping)\n",
        "img_rows, img_cols = 224, 224\n",
        "input_shape = (img_rows, img_cols, 3)"
      ],
      "metadata": {
        "id": "8T5BeJtlAo8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import urllib.request\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# URL of the tar file\n",
        "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ZjXM4RKxlBK9__ZjHBLl5A/aircraft-damage-dataset-v1.tar\"\n",
        "\n",
        "# Define the path to save the file\n",
        "tar_filename = \"aircraft_damage_dataset_v1.tar\"\n",
        "extracted_folder = \"aircraft_damage_dataset_v1\"  # Folder where contents will be extracted\n",
        "\n",
        "# Download the tar file\n",
        "urllib.request.urlretrieve(url, tar_filename)\n",
        "print(f\"Downloaded {tar_filename}. Extraction will begin now.\")\n",
        "\n",
        "# Check if the folder already exists\n",
        "if os.path.exists(extracted_folder):\n",
        "    print(f\"The folder '{extracted_folder}' already exists. Removing the existing folder.\")\n",
        "\n",
        "    # Remove the existing folder to avoid overwriting or duplication\n",
        "    shutil.rmtree(extracted_folder)\n",
        "    print(f\"Removed the existing folder: {extracted_folder}\")\n",
        "\n",
        "# Extract the contents of the tar file\n",
        "with tarfile.open(tar_filename, \"r\") as tar_ref:\n",
        "    tar_ref.extractall()  # This will extract to the current directory\n",
        "    print(f\"Extracted {tar_filename} successfully.\")\n"
      ],
      "metadata": {
        "id": "hMFFqvKFBXzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define directories for train, test, and validation splits\n",
        "extract_path = \"aircraft_damage_dataset_v1\"\n",
        "train_dir = os.path.join(extract_path, 'train')\n",
        "test_dir = os.path.join(extract_path, 'test')\n",
        "valid_dir = os.path.join(extract_path, 'valid')"
      ],
      "metadata": {
        "id": "qHuk1f5SBeUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Data Preprocessing"
      ],
      "metadata": {
        "id": "6XGE7ll8B5_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ImageDataGenerators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "EdmcPc6-BqMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_rows, img_cols),   # Resize images to the size VGG16 expects\n",
        "    batch_size=batch_size,\n",
        "    seed = seed_value,\n",
        "    class_mode='binary',\n",
        "    shuffle=True # Binary classification: dent vs crack\n",
        ")"
      ],
      "metadata": {
        "id": "Ji3CUQj1CWll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_generator=valid_datagen.flow_from_directory(\n",
        "    directory=valid_dir,          # Path to validation data\n",
        "    class_mode='binary',          # Binary classification (damaged vs not damaged)\n",
        "    seed=seed_value,               # Ensures reproducibility\n",
        "    batch_size=batch_size,         # Number of images per batch\n",
        "    shuffle=False,                 # Keep the same order (important for evaluation)\n",
        "    target_size=(img_rows, img_cols)  # Resize images to match model input\n",
        ")"
      ],
      "metadata": {
        "id": "2eLtD8sHC9o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the test data generator\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=test_dir,            # Path to test data\n",
        "    class_mode='binary',           # Binary classification (damaged vs not damaged)\n",
        "    seed=seed_value,               # Ensures reproducibility\n",
        "    batch_size=batch_size,         # Number of images per batch\n",
        "    shuffle=False,                  # Keep the same order for evaluation\n",
        "    target_size=(img_rows, img_cols)  # Resize images to match model input\n",
        ")"
      ],
      "metadata": {
        "id": "tVNU4hs7Dbt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the pre-trained model VGG16"
      ],
      "metadata": {
        "id": "R4YXwHMKDsmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load VGG16 (same)\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n"
      ],
      "metadata": {
        "id": "UIzBWXELDoPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify VGG16 model\n",
        "output = base_model.layers[-1].output\n",
        "output = keras.layers.Flatten()(output)\n",
        "base_model = Model(base_model.input, output)\n",
        "# Freeze the base VGG16 model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "nUPt44KmEHgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the custom model\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "HrSupQadE8EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the model"
      ],
      "metadata": {
        "id": "qAW1Ym9sFX9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ],
      "metadata": {
        "id": "HylKA8vDFHSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training"
      ],
      "metadata": {
        "id": "u6NU5-UQFa5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=n_epochs,\n",
        "    validation_data=valid_generator\n",
        ")"
      ],
      "metadata": {
        "id": "cycatJxlFUd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the training history\n",
        "train_history = model.history.history  # After training"
      ],
      "metadata": {
        "id": "cEYoCG7QJgEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing Training Results\n"
      ],
      "metadata": {
        "id": "q6akh8CPJkzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.plot(train_history['loss'])\n",
        "plt.show()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Validation Loss\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.plot(train_history['val_loss'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "poKyM4H1JiTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Plot accuracy curves for training and validation sets\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(train_history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(train_history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6TwKjhURJpDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation"
      ],
      "metadata": {
        "id": "Fw2EMFjHJ6LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation\n",
        "test_loss, test_accuracy = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "WsZ2nsu8JxvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zSywW6tZJ5jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization function (already provided)\n",
        "def plot_image_with_title(image, model, true_label, predicted_label, class_names):\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(image)\n",
        "    true_label_name = class_names[true_label]\n",
        "    pred_label_name = class_names[predicted_label]\n",
        "    plt.title(f\"True: {true_label_name}\\nPred: {pred_label_name}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def test_model_on_image(test_generator, model, index_to_plot=0):\n",
        "    test_images, test_labels = next(test_generator)\n",
        "    predictions = model.predict(test_images)\n",
        "    predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "    class_indices = test_generator.class_indices\n",
        "    class_names = {v: k for k, v in class_indices.items()}\n",
        "\n",
        "    image_to_plot = test_images[index_to_plot]\n",
        "    true_label = test_labels[index_to_plot]\n",
        "    predicted_label = predicted_classes[index_to_plot]\n",
        "\n",
        "    plot_image_with_title(image=image_to_plot, model=model, true_label=true_label,\n",
        "                         predicted_label=predicted_label, class_names=class_names)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qFbofk54J0UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 7: Visualizing the results\n",
        "test_model_on_image(test_generator, model, index_to_plot=1)"
      ],
      "metadata": {
        "id": "ifRc_GXwOjWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Captioning and Summarization using BLIP Pretrained Model"
      ],
      "metadata": {
        "id": "MdETfMeanKBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the required libraries\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration"
      ],
      "metadata": {
        "id": "iQbZur-pPXVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the pretrained BLIP processor and model:\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
      ],
      "metadata": {
        "id": "U6bhvbIqn5Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Loading BLIP model...\")\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")  # ✅ RENAMED to blip_model\n",
        "print(\"BLIP model loaded successfully!\")"
      ],
      "metadata": {
        "id": "9KbUCm-rpWBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BlipCaptionSummaryLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, processor, model, **kwargs):\n",
        "        \"\"\"\n",
        "        Initialize the custom Keras layer with the BLIP processor and model.\n",
        "\n",
        "        Args:\n",
        "            processor: The BLIP processor for preparing inputs for the model.\n",
        "            model: The BLIP model for generating captions or summaries.\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.processor = processor\n",
        "        self.model = model\n",
        "\n",
        "    def call(self, image_path, task):\n",
        "        # Use tf.py_function to run the custom image processing and text generation\n",
        "        return tf.py_function(self.process_image, [image_path, task], tf.string)\n",
        "\n",
        "    def process_image(self, image_path, task):\n",
        "        \"\"\"\n",
        "        Perform image loading, preprocessing, and text generation.\n",
        "\n",
        "        Args:\n",
        "            image_path: Path to the image file as a string.\n",
        "            task: The type of task (\"caption\" or \"summary\").\n",
        "\n",
        "        Returns:\n",
        "            The generated caption or summary as a string.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Decode the image path from the TensorFlow tensor to a Python string\n",
        "            image_path_str = image_path.numpy().decode(\"utf-8\")\n",
        "\n",
        "            # Open the image using PIL and convert it to RGB format\n",
        "            image = Image.open(image_path_str).convert(\"RGB\")\n",
        "\n",
        "            # Set the appropriate prompt based on the task\n",
        "            if task.numpy().decode(\"utf-8\") == \"caption\":\n",
        "                prompt = \"This is a picture of\"  # Modify prompt for more natural output\n",
        "            else:\n",
        "                prompt = \"This is a detailed photo showing\"  # Modify for summary\n",
        "\n",
        "            # Prepare inputs for the BLIP model\n",
        "            inputs = self.processor(images=image, text=prompt, return_tensors=\"pt\")\n",
        "\n",
        "            # Generate text output using the BLIP model\n",
        "            output = self.model.generate(**inputs)\n",
        "\n",
        "            # Decode the output into a readable string\n",
        "            result = self.processor.decode(output[0], skip_special_tokens=True)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            # Handle errors during image processing or text generation\n",
        "            print(f\"Error: {e}\")\n",
        "            return \"Error processing image\""
      ],
      "metadata": {
        "id": "9YSYmDy-oPV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(image_path, task):\n",
        "    # Create an instance of the custom Keras layer\n",
        "    blip_layer = BlipCaptionSummaryLayer(processor, blip_model)\n",
        "    # Call the layer with the provided inputs\n",
        "    return blip_layer(image_path, task)"
      ],
      "metadata": {
        "id": "k7s3_5okod15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Path to an example image\n",
        "image_path = tf.constant(\"aircraft_damage_dataset_v1/test/dent/144_10_JPG_jpg.rf.4d008cc33e217c1606b76585469d626b.jpg\")  # actual path of image\n",
        "\n",
        "# Generate a caption for the image\n",
        "caption = generate_text(image_path, tf.constant(\"caption\"))\n",
        "# Decode and print the generated caption\n",
        "print(\"Caption:\", caption.numpy().decode(\"utf-8\"))\n",
        "\n",
        "# Generate a summary for the image\n",
        "summary = generate_text(image_path, tf.constant(\"summary\"))\n",
        "# Decode and print the generated summary\n",
        "print(\"Summary:\", summary.numpy().decode(\"utf-8\"))\n"
      ],
      "metadata": {
        "id": "BWtaVNLyoizs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image_url = \"aircraft_damage_dataset_v1/test/dent/149_22_JPG_jpg.rf.4899cbb6f4aad9588fa3811bb886c34d.jpg\"\n",
        "img = plt.imread(image_url)\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# TASK 9: Generate caption for specific image\n",
        "image_path = tf.constant(\"aircraft_damage_dataset_v1/test/dent/149_22_JPG_jpg.rf.4899cbb6f4aad9588fa3811bb886c34d.jpg\")\n",
        "caption = generate_text(image_path, tf.constant(\"caption\"))\n",
        "print(\"Caption:\", caption.numpy().decode(\"utf-8\"))\n",
        "\n",
        "# TASK 10: Generate summary for specific image\n",
        "image_path = tf.constant(\"aircraft_damage_dataset_v1/test/dent/149_22_JPG_jpg.rf.4899cbb6f4aad9588fa3811bb886c34d.jpg\")\n",
        "summary = generate_text(image_path, tf.constant(\"summary\"))\n",
        "print(\"Summary:\", summary.numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "id": "2c45OgkgopAc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}